<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>cs231n作业3笔记 - PzNotes - Learning and Sharing</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="pzhang" />
  <meta name="description" content="简介 在作业 2 中手撸 CNN 后，cs231n 讲述了 RNN，LSTM，GRU 以及语言模型、图像标注、检测、定位、分割、识别等多方面的内容。
" />

  <meta name="keywords" content="Geophysics, Computer Vision, Machine Learning" />






<meta name="generator" content="Hugo 0.69.2" />


<link rel="canonical" href="https://whu-pzhang.github.io/cs231n-assignment3/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">







<link href="/dist/even.min.css?v=3.1.1" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="cs231n作业3笔记" />
<meta property="og:description" content="简介

在作业 2 中手撸 CNN 后，cs231n 讲述了 RNN，LSTM，GRU 以及语言模型、图像标注、检测、定位、分割、识别等多方面的内容。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://whu-pzhang.github.io/cs231n-assignment3/" />
<meta property="article:published_time" content="2018-12-20T00:00:00+00:00" />
<meta property="article:modified_time" content="2018-12-20T00:00:00+00:00" />
<meta itemprop="name" content="cs231n作业3笔记">
<meta itemprop="description" content="简介

在作业 2 中手撸 CNN 后，cs231n 讲述了 RNN，LSTM，GRU 以及语言模型、图像标注、检测、定位、分割、识别等多方面的内容。">
<meta itemprop="datePublished" content="2018-12-20T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2018-12-20T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="4725">



<meta itemprop="keywords" content="Python,NumPy,PyTorch," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="cs231n作业3笔记"/>
<meta name="twitter:description" content="简介

在作业 2 中手撸 CNN 后，cs231n 讲述了 RNN，LSTM，GRU 以及语言模型、图像标注、检测、定位、分割、识别等多方面的内容。"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">PzNotes</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/notes/">
        <li class="mobile-menu-item">Notes</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">PzNotes</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/notes/">Notes</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">cs231n作业3笔记</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-12-20 </span>
        <div class="post-category">
            
              <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"> 计算机视觉 </a>
            
          </div>
        <span class="more-meta"> 约 4725 字 </span>
        <span class="more-meta"> 预计阅读 10 分钟 </span>
        
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  
  <div class="post-toc-content">
    
  </div>
</div>

    
    <div class="post-content">
      <h2 id="简介">简介</h2>

<p>在作业 2 中手撸 CNN 后，cs231n 讲述了 RNN，LSTM，GRU 以及语言模型、图像标注、检测、定位、分割、识别等多方面的内容。</p>

<h2 id="rnn-captioning">RNN captioning</h2>

<p>常规 DNN 和 CNN 只能依次处理一个个的输入，输入之间是完全没有联系的，这对于图像分类的任务来说
是没有问题的，但某些需要处理序列之间关系的任务而言就不适合了。例如：</p>

<ol>
<li>图像标注问题(one to many)： image -&gt; sequence of words</li>
<li>情感分类(many to one): sequence of words -&gt; sentiment</li>
<li>机器翻译(many to many): seq of words -&gt; seq of words</li>
<li>帧级别的视频分类(many to many)</li>
<li>……</li>
</ol>

<p>为了更好地处理序列的信息，RNN(Recurrent Neural Network)诞生了。
RNN 需要预测一个和时间相关的量，其基本架构如下：</p>

<p><figure><img src="/images/rnn.jpg" alt=""></figure></p>

<p>每个时间步都要根据前一个状态和当前输入计算一个新的状态。</p>

<p><span  class="math">\[
\boldsymbol{h}_t = f_W (\boldsymbol{h}_{t-1}, \boldsymbol{x}_t) \\
\downarrow \\
\boldsymbol{h}_t = tanh (\mathbf{W}_{hh}\boldsymbol{h}_{t-1} + \mathbf{W}_{xh} \boldsymbol{x}_t)
\]</span></p>

<p>向上的箭头为每个时间步的输出，是一个 softmax 层</p>

<p><span  class="math">\[
\boldsymbol{y}_t = Softmax (\mathbf{W}_{hy} * \boldsymbol{h}_t)
\]</span></p>

<p>根据上述公式，RNN 单元的 <code>rnn_step_forward</code> 如下：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">prev_hWh <span style="color:#f92672">=</span> prev_h <span style="color:#960050;background-color:#1e0010">@</span> Wh                          <span style="color:#75715e"># (1)</span>
xWx <span style="color:#f92672">=</span> x <span style="color:#960050;background-color:#1e0010">@</span> Wx                                    <span style="color:#75715e"># (2)</span>
hsum <span style="color:#f92672">=</span> prev_hWh <span style="color:#f92672">+</span> xWx <span style="color:#f92672">+</span> b                       <span style="color:#75715e"># (3)</span>
next_h <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>tanh(hsum)                          <span style="color:#75715e"># (4)</span></code></pre></div>
<p>根据计算图反推，backward 也很简单：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">dhsum <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>tanh(hsum)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>) <span style="color:#f92672">*</span> dnext_h      <span style="color:#75715e"># (4)</span>
dprev_hWh <span style="color:#f92672">=</span> dhsum                             <span style="color:#75715e"># (3)</span>
dxWx <span style="color:#f92672">=</span> dhsum                                  <span style="color:#75715e"># (3)</span>
db <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(dhsum, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)                    <span style="color:#75715e"># (3)</span>
dx <span style="color:#f92672">=</span> dxWx <span style="color:#960050;background-color:#1e0010">@</span> Wx<span style="color:#f92672">.</span>T                              <span style="color:#75715e"># (2)</span>
dWx <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>T <span style="color:#960050;background-color:#1e0010">@</span> dxWx                              <span style="color:#75715e"># (2)</span>
dWh <span style="color:#f92672">=</span> prev_h<span style="color:#f92672">.</span>T <span style="color:#960050;background-color:#1e0010">@</span> dprev_hWh                    <span style="color:#75715e"># (1)</span>
dprev_h <span style="color:#f92672">=</span> dprev_hWh <span style="color:#960050;background-color:#1e0010">@</span> Wh<span style="color:#f92672">.</span>T                    <span style="color:#75715e"># (1)</span></code></pre></div>
<p>接下来就是完整的 RNN 了，forward 过程需要对 RNN 单元循环 <code>T</code> 次（<code>T</code>为序列长度），在循环内部每次记得更新 $\boldsymbol{h}_t$ 即可。</p>

<p>backward 过程和以前的神经网络不太一样，RNN 从上游传过来的梯度不只一个，除了从右边传过来的梯度外，
还有每个时间点（上面）传过来的梯度。首先需要逆序循环，然后每个循环内更新 RNN 单元需要的梯度值。
此外，在 RNN 中，Wx, Wh, b 这三个参数是共享的，对每个时间步都是一样的，因此它们的梯度是一个累加的值。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">dprev_ht <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((N, H))
<span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> reversed(range(T)):
    dx[:, t, :], dprev_ht, dWxt, dWht, dbt <span style="color:#f92672">=</span> rnn_step_backward(
        dh[:, t, :] <span style="color:#f92672">+</span> dprev_ht, cache[t])
    dWx <span style="color:#f92672">+=</span> dWxt
    dWh <span style="color:#f92672">+=</span> dWht
    db <span style="color:#f92672">+=</span> dbt
dh0 <span style="color:#f92672">=</span> dprev_ht</code></pre></div>
<p>在用 RNN 对图像进行标注前，还需要进行词嵌入(word embeding)操作。
神经网络不能将单词作为输入，所以需要将单词映射为单词索引的形式。</p>

<p>现在就可以来搭建 RNN 的架构了。首先需要把图像经 CNN 提取的特征（作业中采用的是在 ImageNet 数据集上预训练的 VGG16 模型的 fc7 层提取得到的特征）通过全连接层转换为初始的隐藏状态（$\boldsymbol{h}_0$），接着将 captions 做词嵌入输入 RNN 单元中，再利用一个仿射变换将 RNN 单元的输出转换为单词字典索引，接着采用 softmax 损失计算 loss。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">h0, h0cache <span style="color:#f92672">=</span> affine_forward(features, W_proj, b_proj)
weout, wecache <span style="color:#f92672">=</span> word_embedding_forward(captions_in, W_embed)

<span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>cell_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;rnn&#39;</span>:
    rnnout, rnncache <span style="color:#f92672">=</span> rnn_forward(weout, h0, Wx, Wh, b)

x, xcache <span style="color:#f92672">=</span> temporal_affine_forward(rnnout, W_vocab, b_vocab)
loss, dx <span style="color:#f92672">=</span> temporal_softmax_loss(x, captions_out, mask)</code></pre></div>
<p>上面描述的是 RNN 训练过程，测试时没有真值标注作为输入，需要一个起始单词（<code>&lt;START&gt;</code>）作为开始，然后按序列依次更新隐藏状态，并预测输出下一个标注单词。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">prev_h, _ <span style="color:#f92672">=</span> affine_forward(features, W_proj, b_proj)
captions[:, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_start
x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_start <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>ones((N, ), dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>int32)
<span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, max_length):
    embed, _ <span style="color:#f92672">=</span> word_embedding_forward(x, W_embed)

    <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>cell_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;rnn&#39;</span>:
        next_h, _ <span style="color:#f92672">=</span> rnn_step_forward(embed, prev_h, Wx, Wh, b)

    prev_h <span style="color:#f92672">=</span> next_h
    out, _ <span style="color:#f92672">=</span> affine_forward(next_h, W_vocab, b_vocab)
    idx <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(out, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
    captions[:, t] <span style="color:#f92672">=</span> idx</code></pre></div>
<h2 id="lstm-captioning">LSTM captioning</h2>

<p>RNN 应该能够记住许多时间步之前见过的信息，但实际中由于梯度消失（vanishing gradient problem）问题，
随着层数的增加，网络将变得无法训练。LSTM(Long Short Term Memory)和 GRU 都是为了解决这个问题而提出的。</p>

<p>LSTM 单元结构如下：</p>

<p><figure><img src="./images/lstm.png" alt=""></figure></p>

<p>forward 过程计算公式如下：</p>

<p><span  class="math">\[
\begin{align}
\begin{pmatrix}
i \\
f \\
o \\
g
\end{pmatrix} & =
\begin{pmatrix}
\sigma \\
\sigma \\
\sigma \\
\tanh
\end{pmatrix} \,
\mathbf{W}
\begin{pmatrix}
h*{t-1} \\
x_t
\end{pmatrix} \\
\\
c_t & = f \odot c*{t-1} + i \odot g \\
h_t & = o \odot \tanh(c_t)
\end{align}
\]</span></p>

<p>注意到每个 LSTM 单元有 3 个输入：$c, h, x$。实际计算时 4 个 gate 的线性部分可以通过一次计算完成，然后将不同 gate 对应的值分开即可。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">H <span style="color:#f92672">=</span> prev_h<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
z <span style="color:#f92672">=</span> x <span style="color:#960050;background-color:#1e0010">@</span> Wx <span style="color:#f92672">+</span> prev_h <span style="color:#960050;background-color:#1e0010">@</span> Wh <span style="color:#f92672">+</span> b
igate <span style="color:#f92672">=</span> sigmoid(z[:, :H])
fgate <span style="color:#f92672">=</span> sigmoid(z[:, H:<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> H])
ogate <span style="color:#f92672">=</span> sigmoid(z[:, <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> H:<span style="color:#ae81ff">3</span> <span style="color:#f92672">*</span> H])
ggate <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>tanh(z[:, <span style="color:#ae81ff">3</span> <span style="color:#f92672">*</span> H:])

next_c <span style="color:#f92672">=</span> fgate <span style="color:#f92672">*</span> prev_c <span style="color:#f92672">+</span> igate <span style="color:#f92672">*</span> ggate
next_h <span style="color:#f92672">=</span> ogate <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>tanh(next_c)</code></pre></div>
<p>backward 时，相比 RNN，反传过来的值有两个 <strong><code>dnext_h</code>, <code>dnext_c</code></strong>。求：
<code>dx</code>, <code>dWx</code>, <code>dWh</code>, <code>db</code>, <code>dprev_h</code> 和 <code>dprev_c</code> 的梯度。弄清楚哪些变量之间有关联，然后利用链式法则即可：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">dogate <span style="color:#f92672">=</span> dnext_h <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>tanh(next_c)
dnext_c <span style="color:#f92672">+=</span> dnext_h <span style="color:#f92672">*</span> ogate <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>tanh(next_c)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
dfgate <span style="color:#f92672">=</span> dnext_c <span style="color:#f92672">*</span> prev_c
dprev_c <span style="color:#f92672">=</span> dnext_c <span style="color:#f92672">*</span> fgate
digate <span style="color:#f92672">=</span> dnext_c <span style="color:#f92672">*</span> ggate
dggate <span style="color:#f92672">=</span> dnext_c <span style="color:#f92672">*</span> igate

dz <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((N, <span style="color:#ae81ff">4</span> <span style="color:#f92672">*</span> H))
dz[:, :H] <span style="color:#f92672">=</span> digate <span style="color:#f92672">*</span> igate <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> igate)
dz[:, H:<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> H] <span style="color:#f92672">=</span> dfgate <span style="color:#f92672">*</span> fgate <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.</span> <span style="color:#f92672">-</span> fgate)
dz[:, <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> H:<span style="color:#ae81ff">3</span> <span style="color:#f92672">*</span> H] <span style="color:#f92672">=</span> dogate <span style="color:#f92672">*</span> ogate <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> ogate)
dz[:, <span style="color:#ae81ff">3</span> <span style="color:#f92672">*</span> H:<span style="color:#ae81ff">4</span> <span style="color:#f92672">*</span> H] <span style="color:#f92672">=</span> dggate <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> ggate<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)

dx <span style="color:#f92672">=</span> dz <span style="color:#960050;background-color:#1e0010">@</span> Wx<span style="color:#f92672">.</span>T
dWx <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>T <span style="color:#960050;background-color:#1e0010">@</span> dz
dprev_h <span style="color:#f92672">=</span> dz <span style="color:#960050;background-color:#1e0010">@</span> Wh<span style="color:#f92672">.</span>T
dWh <span style="color:#f92672">=</span> prev_h<span style="color:#f92672">.</span>T <span style="color:#960050;background-color:#1e0010">@</span> dz
db <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(dz, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)</code></pre></div>
<p>完整的 LSTM 大体和 RNN 相同，注意多了一个变量 <code>c</code>，不论是 forward 和 backward 都记得在循环里更新即可。</p>

<h2 id="network-visualization">Network Visualization</h2>

<p>之前训练模型都是利用 SGD 来更新模型参数使其达到损失函数定义下的要求。而在这一节中，我们利用已经训练好的预训练模型来定义相对于图像的损失函数，利用反向传播计算图像像素的梯度，保持模型不变，通过更新图像来最小化损失函数。</p>

<p>主要包括三个方面的内容：</p>

<ul>
<li>显著性图（Saliency Maps）</li>
</ul>

<p>显著性图告诉我们图像中每个像素影响图像类别分数的程度。通过计算正确类未规范化的分数相对于每个像素的梯度来得到。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">scores <span style="color:#f92672">=</span> model(X) <span style="color:#75715e"># correct class scores</span>
scores <span style="color:#f92672">=</span> scores<span style="color:#f92672">.</span>gather(<span style="color:#ae81ff">1</span>, y<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>squeeze() <span style="color:#75715e"># backward</span>
scores<span style="color:#f92672">.</span>backward(torch<span style="color:#f92672">.</span>ones(X<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)))

 <span style="color:#75715e"># image gradient</span>
saliency <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>grad

 <span style="color:#75715e"># absolute the value and take the maximum value over the 3 channels</span>
saliency <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>abs(saliency)
saliency, _ <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>max(saliency, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)</code></pre></div>
<ul>
<li>Fooling images</li>
</ul>

<p>Fooling images 则是在给定图像和类别时，通过梯度上升法不断地更新图像，最后使得模型认为该图像就是这个类别的图像为止。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">while</span> (True):
    scores <span style="color:#f92672">=</span> model(X<span style="color:#f92672">*</span>fooling)
    idx <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(scores)

    <span style="color:#66d9ef">if</span> idx<span style="color:#f92672">.</span>item() <span style="color:#f92672">==</span> target_y:
        <span style="color:#66d9ef">break</span>

    scores[<span style="color:#ae81ff">0</span>, target_y]<span style="color:#f92672">.</span>backward()

    dX <span style="color:#f92672">=</span> X_fooling<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>data

    <span style="color:#75715e"># update image using gradient ascent</span>
    X_fooling<span style="color:#f92672">.</span>data <span style="color:#f92672">+=</span> learning_rate <span style="color:#f92672">*</span> (dX <span style="color:#f92672">/</span> dX<span style="color:#f92672">.</span>norm())

    X_fooling<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>zero_()</code></pre></div>
<ul>
<li>类别可视化（Class Visualization）</li>
</ul>

<p>这里和 fooling images 差不多，都是预训练的模型通过梯度上升将输入的随机噪音图像变为指定类别的图像。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">scores <span style="color:#f92672">=</span> model(img)
loss <span style="color:#f92672">=</span> scores[<span style="color:#ae81ff">0</span>, target_y] <span style="color:#f92672">-</span> l2_reg <span style="color:#f92672">*</span> img<span style="color:#f92672">.</span>norm()
loss<span style="color:#f92672">.</span>backward()
grad <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>data
img<span style="color:#f92672">.</span>data <span style="color:#f92672">+=</span> learning_rate <span style="color:#f92672">*</span> grad
img<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>zero_()</code></pre></div>
<h2 id="style-transfer">Style Transfer</h2>

<p>风格迁移的目的是将参考图像的风格应用于目标图像，同时保留目标图像的内容，从而生成一副新的图像。</p>

<p><figure><img src="./images/20190329.jpg" alt=""></figure></p>

<p>风格迁移的思想与纹理生成的想法密切相关。实现风格迁移背后的关键思想与所有深度学习算法的思想一样：
需要先定义一个损失函数定义要实现的目标，然后采用梯度下降法最小化这个损失函数。
目标就是保存原始图像内容的同时实现风格化，若能在数学上给出 content 和 style 的定义，那么损失函数
可以定义如下：</p>
<pre><code>loss = distance(style(reference_image) - style(generated_image)) +
       distance(content(original_image) - content(generated_image))</code></pre>
<p>这里的 <code>distance</code> 是一个范数，如 L2 范数。<code>content</code> 和 <code>style</code> 分别是计算输入图像内容和风格
的函数。</p>

<h3 id="content-loss">content loss</h3>

<p>内容损失描述的是网络从目标图像提取出的特征图与从生成图像提取的特征图之间的差别。通常选择的是靠近
顶部的某一特征图来进行逐元素比较。代码如下：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">content_loss</span>(content_weight, content_current, content_original):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Compute the content loss for style transfer.
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Inputs:
</span><span style="color:#e6db74">    - content_weight: Scalar giving the weighting for the content loss.
</span><span style="color:#e6db74">    - content_current: features of the current image; this is a PyTorch Tensor of shape
</span><span style="color:#e6db74">      (1, C_l, H_l, W_l).
</span><span style="color:#e6db74">    - content_target: features of the content image, Tensor with shape (1, C_l, H_l, W_l).
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Returns:
</span><span style="color:#e6db74">    - scalar content loss
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    loss <span style="color:#f92672">=</span> content_weight <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>sum((content_current <span style="color:#f92672">-</span> content_original)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
    <span style="color:#66d9ef">return</span> loss</code></pre></div>
<h3 id="style-loss">style loss</h3>

<p>不同于内容损失，风格损失使用卷积网络的多个层来定义，以此来捕捉参考图像和生成图像上不同空间尺度上
的信息。使用 Gram Matrix 来表示不同激活层之间的相关性：我们希望生成图像的激活统计量与风格图像的
激活统计量相匹配。有很多种方法可以来表示这种相关，但 Gram Matrix 易于计算且效果很好。</p>

<p>对于 shape 为 $(C_l, M_l)$ 的特征图 $F^l$ ，其 Gram 矩阵 shape 为 $(C_l, C_l)$:</p>

<p><span  class="math">\[
G_{ij}^l = \sum_k {F_{ik}^l F_{jk}^l}
\]</span></p>

<p>假定 $G^l$ 为当前图像的 Gram 矩阵，$A^l$ 为参考风格图像的特征图 Gram 矩阵，$w_l$ 为权重，
那么 $l$ 层的风格损失可以简单的定义为两个 Gram 矩阵之间的欧氏距离：</p>

<p><span  class="math">\[
L_s^l = w_l \sum_{i,j} {(G_{ij}^l - A_{ij}^l)^2}
\]</span></p>

<p>而风格损失是多个层的损失和：</p>

<p><span  class="math">\[
L_s = \sum_{l \in \mathcal{L}} {L_s^l}
\]</span></p>

<p>首先实现 Gram 矩阵的计算：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gram_matrix</span>(features, normalize<span style="color:#f92672">=</span>True):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Compute the Gram matrix from features.
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Inputs:
</span><span style="color:#e6db74">    - features: PyTorch Tensor of shape (N, C, H, W) giving features for
</span><span style="color:#e6db74">      a batch of N images.
</span><span style="color:#e6db74">    - normalize: optional, whether to normalize the Gram matrix
</span><span style="color:#e6db74">        If True, divide the Gram matrix by the number of neurons (H * W * C)
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Returns:
</span><span style="color:#e6db74">    - gram: PyTorch Tensor of shape (N, C, C) giving the
</span><span style="color:#e6db74">      (optionally normalized) Gram matrices for the N input images.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    N, C, H, W <span style="color:#f92672">=</span> features<span style="color:#f92672">.</span>size()
    features_reshaped <span style="color:#f92672">=</span> features<span style="color:#f92672">.</span>reshape(N, C, H <span style="color:#f92672">*</span> W)
    gram <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>bmm(features_reshaped, features_reshaped<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>))
    <span style="color:#66d9ef">if</span> normalize:
        gram <span style="color:#f92672">/=</span> H <span style="color:#f92672">*</span> W <span style="color:#f92672">*</span> C
    <span style="color:#66d9ef">return</span> gram</code></pre></div>
<p>接着，我们实现风格损失函数：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">style_loss</span>(feats, style_layers, style_targets, style_weights):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Computes the style loss at a set of layers.
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Inputs:
</span><span style="color:#e6db74">    - feats: list of the features at every layer of the current image, as produced by
</span><span style="color:#e6db74">      the extract_features function.
</span><span style="color:#e6db74">    - style_layers: List of layer indices into feats giving the layers to include in the
</span><span style="color:#e6db74">      style loss.
</span><span style="color:#e6db74">    - style_targets: List of the same length as style_layers, where style_targets[i] is
</span><span style="color:#e6db74">      a PyTorch Tensor giving the Gram matrix of the source style image computed at
</span><span style="color:#e6db74">      layer style_layers[i].
</span><span style="color:#e6db74">    - style_weights: List of the same length as style_layers, where style_weights[i]
</span><span style="color:#e6db74">      is a scalar giving the weight for the style loss at layer style_layers[i].
</span><span style="color:#e6db74">      
</span><span style="color:#e6db74">    Returns:
</span><span style="color:#e6db74">    - style_loss: A PyTorch Tensor holding a scalar giving the style loss.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#75715e"># Hint: you can do this with one for loop over the style layers, and should</span>
    <span style="color:#75715e"># not be very much code (~5 lines). You will need to use your gram_matrix function.</span>
    loss <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>FloatTensor([<span style="color:#ae81ff">0</span>])<span style="color:#f92672">.</span>type(dtype)
    <span style="color:#66d9ef">for</span> i, idx <span style="color:#f92672">in</span> enumerate(style_layers):
        gram <span style="color:#f92672">=</span> gram_matrix(feats[idx])
        loss <span style="color:#f92672">+=</span> torch<span style="color:#f92672">.</span>sum(style_weights[i] <span style="color:#f92672">*</span> (gram <span style="color:#f92672">-</span> style_targets[i])<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)       
    <span style="color:#66d9ef">return</span> loss</code></pre></div>
<h3 id="totalvariation-regularization">Total-variation regularization</h3>

<p>除了内容损失和风格损失之外，还需要添加一个全变分正则化损失，使生成图像
的像素具有空间连续性和平滑，避免图像过度像素化。</p>

<p>全变分损失定义为水平和垂直方向相邻像素差的平方和，对于不同的通道之间也是相加。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tv_loss</span>(img, tv_weight):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Compute total variation loss.
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Inputs:
</span><span style="color:#e6db74">    - img: PyTorch Variable of shape (1, 3, H, W) holding an input image.
</span><span style="color:#e6db74">    - tv_weight: Scalar giving the weight w_t to use for the TV loss.
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Returns:
</span><span style="color:#e6db74">    - loss: PyTorch Variable holding a scalar giving the total variation loss
</span><span style="color:#e6db74">      for img weighted by tv_weight.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#75715e"># Your implementation should be vectorized and not require any loops!</span>
    hsum <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sum((img[:,:,:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,:] <span style="color:#f92672">-</span> img[:,:,<span style="color:#ae81ff">1</span>:,:])<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
    wsum <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sum((img[:,:,:,:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">-</span> img[:,:,:,<span style="color:#ae81ff">1</span>:])<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
    <span style="color:#66d9ef">return</span> tv_weight <span style="color:#f92672">*</span> (hsum <span style="color:#f92672">+</span> wsum)</code></pre></div>
<p>在风格迁移中，我们需要最小化的损失为内容损失、风格损失和全变差损失这三部分之和。
接下来，只需要将这三部分组合起来，利用梯度下降法来得到最后的图像即可。具体可以参考
<a href="https://github.com/whu-pzhang/cs231n/blob/master/assignment3/StyleTransfer-PyTorch.ipynb">StyleTransfer-Pytorch.ipynb</a></p>

<h2 id="gan">GAN</h2>

<p>在此之前，cs231n中所有的神经网络都是判别式模型，从给定的输入得到一个类别标记输出。其应用范围从图像分类
到生成图像描述。在这一小节中，我们利用神经网络来构建生成模型。</p>

<p>2014年，<a href="https://arxiv.org/abs/1406.2661">Goodfellow et al.</a>提出了训练生成模型的生成对抗网络
（Generative Adversarial Networks，GAN）。在该方法中，需要构建两个不同的网络：
第一个网络是传统的分类网络，称为判别器（discriminator），其功能是判断输入图像是真实的（输入训练集）
还是假的（不属于训练集）。第二个网络称为生成器（generator），将随机噪声作为输入并产生一张图像。
生成器的目标是骗过判别器，使其认为其生成的图像是真实的。</p>

<p>可以把判别器和生成器的博弈看作是一个最小最大的过程：</p>

<p><span  class="math">\[
\min_{G} \max_{D} \mathbb{E}_{x~p_{data}} \left[ \log D(x) \right] + \mathbb{E}_{z~p(z)} \left[ \log (1-D(G(z))) \right]
\]</span></p>

<p>其中，$z~p(z)$ 为随机噪声样本，$G(z)$ 为生成器 $G$ 产生的图像，$D$ 为判别器的输出，表示输入为
真实图像的概率。在<a href="https://arxiv.org/abs/1406.2661">Goodfellow et al.</a>中，作者分析了这个
最小最大过程和最小化训练数据分布与生成样本之间Jensen-Shannon散度的关联。</p>

<p>为了优化这个最小最大过程，可以选择对 $G$ 进行梯度下降优化，对 $D$ 进行梯度上升优化：</p>

<ol>
<li>更新生成器 $G$ 使得判别器作出正确判断的概率最小</li>
<li>更新判别器 $D$ 使得判别器作出正确判断的概率最大</li>
</ol>

<p>但是上述两个过程实际中难以work。因此，实际中<strong>更新生成器的准则是最大化判别器作出错误判断的概率</strong>。</p>

<p>该小节中，我们将交替执行如下更新：</p>

<ol>
<li>最大化判别器作出错误判断的概率来更新生成器 $G$:</li>
</ol>

<p><span  class="math">\[
\max_{G} \mathbb{E}_{z~p(z)} \left[ \log D(G(z)) \right]
\]</span></p>

<ol start="2">
<li>最大化判别器在真实数据和生成数据上作出正确判断的概率来更新判别器 $D$</li>
</ol>

<p><span  class="math">\[
\max_{D} \mathbb{E}_{x~p_{data}} \left[ \log D(x) \right] + \mathbb{E}_{z~p_{z}} \left[ \log (1-D(G(z))) \right]
\]</span></p>

<p>实际上，自2014年GAN提出以来，有关GAN的论文层出不穷，相较于其他的生成模型，GAN能生成质量最高图像的同时，
也需要高超的训练技巧。这个<a href="https://github.com/soumith/ganhacks">repo</a>里包含了训练GAN的17个trick。
提升GAN训练的稳定性和鲁棒性是一个开放的研究领域，每天都会用小的paper出来。最近的GAN教程，可以看
<a href="https://arxiv.org/abs/1701.00160">这里</a>。
最近将目标函数变为Wasserstein距离的GAN模型（<a href="https://arxiv.org/abs/1701.07875">WGAN</a>，<a href="https://arxiv.org/abs/1704.00028">WGAN-GP</a>）获得了更稳定的结果。</p>

<p>需要注意的是，GAN不是训练生成模型的唯一方法！另一个流行的方法是变分自编码器（Variational Autoencoders）
（由<a href="https://arxiv.org/abs/1312.6114">here</a> 和 <a href="https://arxiv.org/abs/1401.4082">here</a>共同发现）。VAE易于训练，但其生成的图像质量远不如GAN。</p>

<p>GAN的损失函数是用BCE（Binary Cross-Entropy）定义的，也即是对数回归的损失函数：</p>

<p><span  class="math">\[
bce(s,y) = -y * \log(s) - (1-y) * \log(1-s)
\]</span></p>

<p>这里的$s$为经sigmoid函数作用后各个类别的分数，即 $s = \sigma(x)$，其值在0～1之间。那么直接
计算bce loss 的时候就会有数值不稳定的问题：若s的值很小，那么 $\log(s)$ 便会接近 $-\infty$。
因此，需做如下优化：</p>

<p><span  class="math">\[
\begin{align}
& -y \log(\sigma(x)) - (1-y) \log(1 - \sigma(x)) \\
&= -y \log(\frac{1}{1+e^{-x}}) - (1-y) \log(e^{-x} - \frac{1}{1+e^{-x}}) \\
&= y \log(1 + e^{-x}) + (1 - y) (x + \log(1 + e^{-x})) \\
&= (1 - y) x + \log(1 + e^{-x}) \\
&= x - xy + \log(1 + e^{-x})  % 避免 x < 0时， exp(-x) 溢出 \\
&= -xy + \log(1 + e^x)
\end{align}
\]</span></p>

<p>为了确保稳定性，实现如下：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">bce_loss</span>(input, target):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Numerically stable version of the binary cross-entropy loss function.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    As per https://github.com/pytorch/pytorch/issues/751
</span><span style="color:#e6db74">    See the TensorFlow docs for a derivation of this formula:
</span><span style="color:#e6db74">    https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    Inputs:
</span><span style="color:#e6db74">    - input: PyTorch Tensor of shape (N, ) giving scores.
</span><span style="color:#e6db74">    - target: PyTorch Tensor of shape (N,) containing 0 and 1 giving targets.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    Returns:
</span><span style="color:#e6db74">    - A PyTorch Tensor containing the mean BCE loss over the minibatch of input data.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    neg_abs <span style="color:#f92672">=</span> <span style="color:#f92672">-</span> input<span style="color:#f92672">.</span>abs()
    loss <span style="color:#f92672">=</span> input<span style="color:#f92672">.</span>clamp(min<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>) <span style="color:#f92672">-</span> input <span style="color:#f92672">*</span> target <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> neg_abs<span style="color:#f92672">.</span>exp())<span style="color:#f92672">.</span>log()
    <span style="color:#66d9ef">return</span> loss<span style="color:#f92672">.</span>mean()</code></pre></div>
<p>在此基础上，便可以分别实现判别器和生成器的损失函数了。这里就不贴代码了。
后面的部分是介绍不同的GAN以及实现了。作业可以参考我的repo：<a href="https://github.com/whu-pzhang/cs231n/blob/master/assignment3/GANs-PyTorch.ipynb">GANs-Pytorch.ipynb</a></p>

<h2 id="总结">总结</h2>

<p>作业3的内容更多的是将深度学习的多个应用展示一下，然后通过作业的形式来熟悉深度学习框架。
涉及到的内容很丰富，想要深入理解的话，需要自己去查看相关文献才行。</p>

<p>至此，cs231n的作业笔记全部完成！🎉</p>
    </div>

    
    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">pzhang</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">2018-12-20</span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">许可协议</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a></span>
  </p>
</div>

    
    

    <footer class="post-footer">
      <div class="post-tags">
          
          <a href="/tags/python/">Python</a>
          
          <a href="/tags/numpy/">NumPy</a>
          
          <a href="/tags/pytorch/">PyTorch</a>
          
        </div>

      
      <nav class="post-nav">
        
        
          <a class="next" href="/cs231n-assignment2/">
            <span class="next-text nav-default">cs231n作业2笔记</span>
            <span class="prev-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:pzhang.omega@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="https://github.com/whu-pzhang" class="iconfont icon-github" title="github"></a>
  <a href="https://whu-pzhang.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    
      2014 - 
    2020
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">whu-pzhang</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>
<script type="text/javascript" src="/dist/even.min.js?v=3.1.1"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
